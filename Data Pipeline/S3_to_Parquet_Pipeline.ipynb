{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a240081e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill an s3 bucket with processed historical data while this notebook runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "62a755f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: EDA on getads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39ddc8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "from io import StringIO\n",
    "import gc\n",
    "from datetime import datetime\n",
    "import farmhash\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import gzip\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d0cad57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_from_prefix(prefix):\n",
    "    if 'notify' in prefix:\n",
    "        return 'Notification'\n",
    "    elif 'adreturned' in prefix:\n",
    "        return 'Response'\n",
    "    elif 'non_impressed_requests' in prefix or 'getads' in prefix:\n",
    "        return 'Request'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "            \n",
    "os_categories = ['Apple', 'Windows', 'Google', 'Linux', 'TV', 'Other']\n",
    "publisher_categories = ['appnexus', 'google', 'openx', 'rubicon', 'Other']\n",
    "\n",
    "def preprocess_os(os):\n",
    "    if os != os: #NaN satisfies this property\n",
    "        return os_categories[-1]\n",
    "    if os in ['iOS', 'iPadOS', 'Apple iOS', 'Apple Mac', 'Macintosh', 'OS X', 'MacOS', 'X11']:\n",
    "        return os_categories[0]\n",
    "    elif 'Windows' in os :\n",
    "        return os_categories[1]\n",
    "    elif os in ['Chrome OS']:\n",
    "        return os_categories[2]\n",
    "    elif os in ['Linux']:\n",
    "        return os_categories[3]\n",
    "    \n",
    "    elif os in ['LG proprietary', 'webOS', 'Tizen', 'tvOS', 'Roku OS']:\n",
    "        return os_categories[4]\n",
    "    else:\n",
    "        return os_categories[-1]\n",
    "    \n",
    "    \n",
    "def preprocess_publisher(publisher):\n",
    "    if publisher in publisher_categories:\n",
    "        return publisher\n",
    "    else:\n",
    "        return publisher_categories[-1] \n",
    "    \n",
    "# create one hot encoders\n",
    "os_encoder = OneHotEncoder(categories=[os_categories], sparse_output=False)\n",
    "publisher_encoder = OneHotEncoder(categories=[publisher_categories], sparse_output=False)\n",
    "\n",
    "os_encoder.fit(np.array(os_categories).reshape(-1, 1))\n",
    "publisher_encoder.fit(np.array(publisher_categories).reshape(-1, 1))\n",
    "\n",
    "def process_row(row, sheet_name):\n",
    "\n",
    "    if sheet_name == 'Request':\n",
    "        row = row[['timestamp','LatUsed','LongUsed','PublisherName','BundleId','os','bidfloor','w','h']]\n",
    "        \n",
    "    elif sheet_name == 'Notification':\n",
    "        row['w'], row['h'] = map(int, row['CreativeSize'].split('x'))\n",
    "        row = row[['timestamp','LatUsed','LongUsed','PublisherName','bundleid','OS','BidFloor','w','h']]\n",
    "    \n",
    "    # parsing the datetime string\n",
    "    date_object = datetime.strptime(row[0], \"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # one-hot encoding os and publisher_name\n",
    "    os_one_hot = os_encoder.transform([[preprocess_os(row[5])]]).tolist()[0]\n",
    "    publisher_one_hot = publisher_encoder.transform([[preprocess_publisher(row[3])]]).tolist()[0]\n",
    "\n",
    "    # Continuous values\n",
    "    continuous_input_val = [\n",
    "        date_object.weekday(),  # day of the week\n",
    "        date_object.month,  # month\n",
    "        date_object.hour,  # hour\n",
    "        row[1],  # userLong\n",
    "        row[2],  # userLat\n",
    "        row[6],  # bidfloor\n",
    "        row[7],   # creativeWidth\n",
    "        row[8]   # creativeHeight\n",
    "    ]\n",
    "    \n",
    "    # One-hot encoded values\n",
    "    ohe_input_val = os_one_hot + publisher_one_hot\n",
    "    \n",
    "    # Hashed website name\n",
    "    if not pd.isna(row[4]):\n",
    "        embedding_input_val = [farmhash.hash64(row[4]) % 10000]\n",
    "    else: \n",
    "        embedding_input_val = row[4]\n",
    "    \n",
    "    return continuous_input_val, ohe_input_val, embedding_input_val\n",
    "\n",
    "def process_file(bucket, obj, prefix):\n",
    "    \"\"\"Process a single CSV file in an S3 bucket and return processed data.\"\"\"\n",
    "    processed_rows = []  # List to accumulate processed data\n",
    "    \n",
    "    sheet_name = get_label_from_prefix(prefix)\n",
    "    log_format_df = pd.read_excel('LogFormat 1.8.xlsx', sheet_name) #Always make sure this is updated!\n",
    "    gzipped_binary_data = obj.get()['Body'].read()\n",
    "    decompressed_binary_data = gzip.decompress(gzipped_binary_data)\n",
    "    text_data = StringIO(decompressed_binary_data.decode('ISO-8859-1'))\n",
    "    data = pd.read_csv(text_data, header=None, names=log_format_df['Field Name'], low_memory=False)\n",
    "    \n",
    "    for index, row in data.iterrows():        \n",
    "        processed_row = process_row(row, sheet_name)\n",
    "        processed_rows.append(processed_row)  # Add processed row to the list\n",
    "    \n",
    "    lst = [data]\n",
    "    del data\n",
    "    del lst\n",
    "    gc.collect()\n",
    "    \n",
    "    return processed_rows\n",
    "\n",
    "def get_processed_keys_from_s3_ledger(bucket_name, ledger_key):\n",
    "    \"\"\"Read the processed keys from the S3 ledger file.\"\"\"\n",
    "    s3 = boto3.resource('s3')\n",
    "    try:\n",
    "        ledger_data = s3.Object(bucket_name, ledger_key).get()['Body'].read().decode('utf-8')\n",
    "        return set(line.strip() for line in ledger_data.splitlines())\n",
    "    except s3.meta.client.exceptions.NoSuchKey:\n",
    "        # Ledger file doesn't exist yet\n",
    "        return set()\n",
    "\n",
    "def append_key_to_s3_ledger(bucket_name, ledger_key, key):\n",
    "    \"\"\"Append a processed key to the S3 ledger.\"\"\"\n",
    "    current_keys = get_processed_keys_from_s3_ledger(bucket_name, ledger_key)\n",
    "    current_keys.add(key)\n",
    "    \n",
    "    # Now, write the updated set of keys back to S3\n",
    "    s3 = boto3.resource('s3')\n",
    "    s3.Object(bucket_name, ledger_key).put(Body=\"\\n\".join(current_keys))\n",
    "    \n",
    "def process_and_transfer_data(source_bucket_name, destination_bucket_name, ledger_key, prefixes, processing_function):\n",
    "    \"\"\"\n",
    "    Processes data from source_bucket using the provided processing function \n",
    "    and saves the processed data to the destination_bucket.\n",
    "    \n",
    "    Args:\n",
    "    - source_bucket_name (str): Name of the source S3 bucket.\n",
    "    - destination_bucket_name (str): Name of the destination S3 bucket.\n",
    "    - prefixes (list): List of prefixes to process in the source bucket.\n",
    "    - processing_function (callable): Function to process the data.\n",
    "    \"\"\"\n",
    "    \n",
    "    s3 = boto3.resource('s3')\n",
    "    source_bucket = s3.Bucket(source_bucket_name)\n",
    "    \n",
    "    # Get the set of processed keys from the S3 ledger\n",
    "    processed_keys = get_processed_keys_from_s3_ledger(destination_bucket_name, ledger_key)\n",
    "    \n",
    "    for prefix in prefixes:\n",
    "        for obj in source_bucket.objects.filter(Prefix=prefix):\n",
    "            if obj.key not in processed_keys:\n",
    "                # Process the data\n",
    "                processed_data_list = processing_function(source_bucket, obj, prefix)\n",
    "                \n",
    "                data = {\n",
    "                    'continuous': [item[0] for item in processed_data_list],\n",
    "                    'one-hot-encoded': [item[1] for item in processed_data_list],\n",
    "                    'embedding': [item[2] for item in processed_data_list]\n",
    "                }\n",
    "\n",
    "                # Convert the list of processed data to DataFrame\n",
    "                df = pd.DataFrame(data)\n",
    "                \n",
    "                # Convert the processed data DataFrame to Parquet format\n",
    "                parquet_buffer = BytesIO()\n",
    "                df.to_parquet(parquet_buffer)\n",
    "\n",
    "                # Clean up DataFrame to free memory\n",
    "                lst = [df]\n",
    "                del df\n",
    "                del lst\n",
    "                gc.collect()\n",
    "                \n",
    "                # Define the destination key\n",
    "                parts = obj.key.split('/')\n",
    "                event_type, year, month, day = parts[1], parts[2], parts[3], parts[4]\n",
    "                new_key = f'{event_type}/{year}/{month}/{day}/{str(uuid.uuid4())}.parquet'  # Adding .parquet extension\n",
    "\n",
    "                # Upload the Parquet data to the destination bucket\n",
    "                destination_bucket = s3.Bucket(destination_bucket_name)\n",
    "                destination_bucket.put_object(Key=new_key, Body=parquet_buffer.getvalue())\n",
    "\n",
    "                # Append this object key to the S3 ledger\n",
    "                append_key_to_s3_ledger(destination_bucket_name, ledger_key, obj.key)\n",
    "            else:\n",
    "                print(f\"Object {obj.key} has already been processed. Skipping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625168d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes = ['reporting_logs/notify/2023/08/28','reporting_logs/getads/2023/08/28']\n",
    "process_and_transfer_data('cdigital-logs', 'chalk-sm-processing-data', 'ledger/processed_files.txt', prefixes, process_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ed5547",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
